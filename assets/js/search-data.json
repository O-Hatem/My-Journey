{
  
    
        "post0": {
            "title": "Garbage Collection in Python",
            "content": "import datatable as dt import pandas as pd . data = dt.fread(&#39;car-sales.csv&#39;).to_pandas() data.head() . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . 1 Honda | Red | 87899 | 4 | $5,000.00 | . 2 Toyota | Blue | 32549 | 3 | $7,000.00 | . 3 BMW | Black | 11179 | 5 | $22,000.00 | . 4 Nissan | White | 213095 | 4 | $3,500.00 | . data_2 = data . print(&quot;data id&quot;, id(data)) print(&quot;data_2 id&quot;, id(data_2)) . data id 140536166553728 data_2 id 140536166553728 . it&#39;s the same address, so changing in one means change in the other . data[&#39;test_1&#39;] = &#39;test_1&#39; . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . but using the copy() method it creates a variable with a different address . data_3 = data.copy() data_3.head(1) . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . print(&quot;data id&quot;, id(data)) print(&quot;data_2 id&quot;, id(data_2)) print(&quot;data_3 id&quot;, id(data_3)) . data id 140536166553728 data_2 id 140536166553728 data_3 id 140534800707936 . data_2[&quot;test_2&quot;] = &quot;test_2&quot; . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_1 test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | test_2 | . Make Colour Odometer (KM) Doors Price test_1 test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | test_2 | . Changing a table inplace means it doesn&#39;t change its address. It works with the same allocated memory . data.drop(&#39;test_1&#39;, axis=1, inplace=True) . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . But assigning it to a new variable means a new memory slot needs to be allocated . data = data.drop(&#39;test_2&#39;, axis=1) . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . What if we split the data does it change the address? . X = data.drop(&#39;Price&#39;, axis=1) y = data[&#39;Price&#39;] . print(&quot;X id&quot;, id(X)) print(&quot;y id&quot;, id(y)) print(&quot;data id&quot;, id(data)) . X id 140534800254576 y id 140534800254288 data id 140534800254048 . Trying it with simple variables . a = 1 b = a c = a + 1 print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) a = a+1 print() print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) . a id 140536221767984 b id 140536221767984 c id 140536221768016 a id 140536221768016 b id 140536221767984 c id 140536221768016 . a = 1 b = a c = a + 1 print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) a = b+1 print() print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) . a id 140536221767984 b id 140536221767984 c id 140536221768016 a id 140536221768016 b id 140536221767984 c id 140536221768016 . so any thing in the the rhs will evaluate as a new variable, therefore new memory address . Just noticed that increasing the value of a by 1 (now it&#39;s 2, the same as c) will make the address of a and c the same. . Does that mean Python will create one memory slot for a specific value and any variable with that value will get the same address? . d = 2 print(&quot;d id&quot;, id(d)) . d id 140536221768016 . it looks like it. i&#39;m impressed ngl . i read a little into an article about python garbage collection. . What i got is, yes, it allocate only one memory slot for the object and in the underlying layers it got something called reference count, where it keeps count of all objects uses this memory slot and when the count is equal to 0 it deallocate it immediatly. . import sys sys.getrefcount(2) e = 2 sys.getrefcount(2) del e sys.getrefcount(2) . 1928 . 1929 . 1928 . from this article . After using this type of statement, the objects are no longer accessible for the given code. But, the objects are still there in the memory. . so we use gc.collect() after it to free the memory. . import gc . a = 19 del a gc.collect() . 0 .",
            "url": "https://o-hatem.github.io/My-Journey-/2021/11/16/Garbage-collection-in-Python.html",
            "relUrl": "/2021/11/16/Garbage-collection-in-Python.html",
            "date": " • Nov 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Categorical Encodings",
            "content": "About . Most Machine Learning algorithms can&#39;t make use of categorical features untill they are converted into numerical values. that&#39;s where &quot;Categorical Encoding&quot; comes into play. There are a lot of different ways to convert categorical features, some are better than other in different situations. I&#39;ll be doing my best to clarify how each categorical encoding work and when to use them. . Categorical Encoding can be divided into two broad categories, Nominal (there&#39;s no order to the categories) and Oridnal (there&#39;s some order into them). . Examples for Nominal: . Red, Blue, Black | Car, Ship, Plane | . Examples for Ordinal: . Excellent, Very Good, Good, Failed | Tall, medium, short | . the most stable and accurate encoders are target-based encoders with Double Validation: Catboost Encoder, James-Stein Encoder, M-estimator Encoder and Target Encoder | . Using Single Validation will result in much better outcome than not using validation at all. Double validation will achieve more stable score but it would costs more resources and time. | . Regularization is a must for target-based encoders. | . Reference y, y+ = # of target values, # of true target variable | n, n+ = # of observations for a given value in a categoricla column | xi, yi = ith value of category and target | a = regularization hyperparameter, default is prior(mean value of the target) | . | . is_binary_classification = False # set to True, if you data is normally distributed (for James-Stein Encoding) is_normally_distributed = False . Data . The data we&#39;ll be using is from ZeroToMastery: Machine Learning and Data Science Udemy Course. . data = pd.read_csv(&#39;https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales.csv&#39;) data . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . 1 Honda | Red | 87899 | 4 | $5,000.00 | . 2 Toyota | Blue | 32549 | 3 | $7,000.00 | . 3 BMW | Black | 11179 | 5 | $22,000.00 | . 4 Nissan | White | 213095 | 4 | $3,500.00 | . 5 Toyota | Green | 99213 | 4 | $4,500.00 | . 6 Honda | Blue | 45698 | 4 | $7,500.00 | . 7 Honda | Blue | 54738 | 4 | $7,000.00 | . 8 Toyota | White | 60000 | 4 | $6,250.00 | . 9 Nissan | White | 31600 | 4 | $9,700.00 | . data[&#39;Price&#39;] = data[&#39;Price&#39;].str.replace(&#39;$&#39;, &#39;&#39;).str.replace(&#39;,&#39;, &#39;&#39;).str.replace(&#39;.&#39;, &#39;&#39;).astype(int)/100 data.drop(&#39;Colour&#39;, axis=1, inplace=True) data . Make Odometer (KM) Doors Price . 0 Toyota | 150043 | 4 | 4000.0 | . 1 Honda | 87899 | 4 | 5000.0 | . 2 Toyota | 32549 | 3 | 7000.0 | . 3 BMW | 11179 | 5 | 22000.0 | . 4 Nissan | 213095 | 4 | 3500.0 | . 5 Toyota | 99213 | 4 | 4500.0 | . 6 Honda | 45698 | 4 | 7500.0 | . 7 Honda | 54738 | 4 | 7000.0 | . 8 Toyota | 60000 | 4 | 6250.0 | . 9 Nissan | 31600 | 4 | 9700.0 | . data[&#39;Make&#39;].value_counts() . Toyota 4 Honda 3 Nissan 2 BMW 1 Name: Make, dtype: int64 . One Hot Encoding . It uses a vector to denote the absence or existence for a category. the length of the vector depends on the number of categories in the feature. | . It creates N columns (N is the number of categories). | . When solving regression problems it&#39;s better to keep the columns at N-1 to ensure the correct number of degrees of freedom (N-1) | . It&#39;s recommended to Use N columns for classifiction, espically when using a tree-based algorithm. | . It&#39;s recommended to use N-1 columns for algorithms that look at all the features simultaneoulsy during training (e.g. SVM, NN, clustring algorithms). | . If the number of categories is big, it will slow the training process. OHE expands the size of your dataset, which makes it memory-inefficient encoder. | . There are several strategies to overcome the memory problem with OHE, one of which is working with sparse not dense data representation.?? | . ohe = ce.OneHotEncoder(cols=&#39;Make&#39;, use_cat_names=True) one_hot_encoded_data = ohe.fit_transform(data) one_hot_encoded_data . Make_Toyota Make_Honda Make_BMW Make_Nissan Odometer (KM) Doors Price . 0 1 | 0 | 0 | 0 | 150043 | 4 | 4000.0 | . 1 0 | 1 | 0 | 0 | 87899 | 4 | 5000.0 | . 2 1 | 0 | 0 | 0 | 32549 | 3 | 7000.0 | . 3 0 | 0 | 1 | 0 | 11179 | 5 | 22000.0 | . 4 0 | 0 | 0 | 1 | 213095 | 4 | 3500.0 | . 5 1 | 0 | 0 | 0 | 99213 | 4 | 4500.0 | . 6 0 | 1 | 0 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 1 | 0 | 0 | 54738 | 4 | 7000.0 | . 8 1 | 0 | 0 | 0 | 60000 | 4 | 6250.0 | . 9 0 | 0 | 0 | 1 | 31600 | 4 | 9700.0 | . Sum Encoding . also known as Deviation encoding or Effect encoding . | Sum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models. . | However, the difference between them is the interpretation of LR coefficients.?? . OHE model the intercept represents the mean for the baseline condition and coefficients represents simple effects (the difference between one particular condition and the baseline), | in Sum Encoder model the intercept represents the grand mean (across all conditions) and the coefficients can be interpreted directly as the main effects. | . | . sum_encoding = ce.SumEncoder(cols=[&#39;Make&#39;]) sum_encoded_data = sum_encoding.fit_transform(data).drop(&#39;intercept&#39;, axis=1) sum_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 1.0 | 0.0 | 0.0 | 150043 | 4 | 4000.0 | . 1 0.0 | 1.0 | 0.0 | 87899 | 4 | 5000.0 | . 2 1.0 | 0.0 | 0.0 | 32549 | 3 | 7000.0 | . 3 0.0 | 0.0 | 1.0 | 11179 | 5 | 22000.0 | . 4 -1.0 | -1.0 | -1.0 | 213095 | 4 | 3500.0 | . 5 1.0 | 0.0 | 0.0 | 99213 | 4 | 4500.0 | . 6 0.0 | 1.0 | 0.0 | 45698 | 4 | 7500.0 | . 7 0.0 | 1.0 | 0.0 | 54738 | 4 | 7000.0 | . 8 1.0 | 0.0 | 0.0 | 60000 | 4 | 6250.0 | . 9 -1.0 | -1.0 | -1.0 | 31600 | 4 | 9700.0 | . Helmert Encoding . Helmert coding is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. | . This type of encoding can be useful in certain situations where levels of the categorical variable are ordered, say, from lowest to highest, or from smallest to largest.The mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels. source . | . helmert_encoding = ce.HelmertEncoder(cols=&#39;Make&#39;, drop_invariant=True) helmert_encoded_data = helmert_encoding.fit_transform(data) helmert_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 -1.0 | -1.0 | -1.0 | 150043 | 4 | 4000.0 | . 1 1.0 | -1.0 | -1.0 | 87899 | 4 | 5000.0 | . 2 -1.0 | -1.0 | -1.0 | 32549 | 3 | 7000.0 | . 3 0.0 | 2.0 | -1.0 | 11179 | 5 | 22000.0 | . 4 0.0 | 0.0 | 3.0 | 213095 | 4 | 3500.0 | . 5 -1.0 | -1.0 | -1.0 | 99213 | 4 | 4500.0 | . 6 1.0 | -1.0 | -1.0 | 45698 | 4 | 7500.0 | . 7 1.0 | -1.0 | -1.0 | 54738 | 4 | 7000.0 | . 8 -1.0 | -1.0 | -1.0 | 60000 | 4 | 6250.0 | . 9 0.0 | 0.0 | 3.0 | 31600 | 4 | 9700.0 | . helmert_encoding.mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: Make_0 Make_1 Make_2 1 -1.0 -1.0 -1.0 2 1.0 -1.0 -1.0 3 0.0 2.0 -1.0 4 0.0 0.0 3.0 -1 0.0 0.0 0.0 -2 0.0 0.0 0.0}] . Label Encoding . In this encoding, it assigns a number for each category, ranging from 1 to N. . The major issue with it is the numbers don&#39;t necessarly represent an order (Toyota &gt; Honda &gt; BMW &gt; Nissan). . from sklearn.preprocessing import LabelEncoder label_encoding = LabelEncoder() label_encoded_data = data.copy() label_encoded_data[&#39;Make&#39;] = label_encoding.fit_transform(data[&#39;Make&#39;]) label_encoded_data . Make Odometer (KM) Doors Price . 0 3 | 150043 | 4 | 4000.0 | . 1 1 | 87899 | 4 | 5000.0 | . 2 3 | 32549 | 3 | 7000.0 | . 3 0 | 11179 | 5 | 22000.0 | . 4 2 | 213095 | 4 | 3500.0 | . 5 3 | 99213 | 4 | 4500.0 | . 6 1 | 45698 | 4 | 7500.0 | . 7 1 | 54738 | 4 | 7000.0 | . 8 3 | 60000 | 4 | 6250.0 | . 9 2 | 31600 | 4 | 9700.0 | . label_encoding.classes_ . array([&#39;BMW&#39;, &#39;Honda&#39;, &#39;Nissan&#39;, &#39;Toyota&#39;], dtype=object) . Ordinal Encoding . It mostly works the same way as Label Encoding. Label encoding wouldn&#39;t consider whether the feature is ordinal or not. With ordinal encoding we provide what is the order of the categories in a column. . mapping = [{&quot;col&quot;: &quot;Make&quot;, &quot;mapping&quot;: {&quot;BMW&quot;: 1, &quot;Honda&quot;: 2, &quot;Nissan&quot;: 3, &quot;Toyota&quot;: 4}}] ordinal_encoding = ce.OrdinalEncoder(cols=[&quot;Make&quot;], mapping=mapping) ordinal_encoded_data = ordinal_encoding.fit_transform(data) ordinal_encoded_data . Make Odometer (KM) Doors Price . 0 4 | 150043 | 4 | 4000.0 | . 1 2 | 87899 | 4 | 5000.0 | . 2 4 | 32549 | 3 | 7000.0 | . 3 1 | 11179 | 5 | 22000.0 | . 4 3 | 213095 | 4 | 3500.0 | . 5 4 | 99213 | 4 | 4500.0 | . 6 2 | 45698 | 4 | 7500.0 | . 7 2 | 54738 | 4 | 7000.0 | . 8 4 | 60000 | 4 | 6250.0 | . 9 3 | 31600 | 4 | 9700.0 | . ordinal_encoding.category_mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: {&#39;BMW&#39;: 1, &#39;Honda&#39;: 2, &#39;Nissan&#39;: 3, &#39;Toyota&#39;: 4}}] . Label Encoding + Ordinal Encoding . such transformation should not be used “as is” for several types of models (Linear Models, KNN, Neural Nets, etc.). . | While applying gradient boosting it could be used only if the type of a column is specified as “category” . | . df[“category_representation”] = df[“category_representation”].astype(“category”) . If you are working with tabular data and your model is gradient boosting (especially LightGBM library), LE is the simplest and efficient way for you to work with categories in terms of memory (the category type in python consumes much less memory than the object type). | . Binary Encoding . Binary Encoding converts the number of categories N into a binary number, and uses every bit as a column. Here we have 4 categories which can be stored in 3 bits, meaning 3 columns. . binary_encoding = ce.BinaryEncoder(cols=&#39;Make&#39;) binary_encoded_data = binary_encoding.fit_transform(data) binary_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 0 | 0 | 1 | 150043 | 4 | 4000.0 | . 1 0 | 1 | 0 | 87899 | 4 | 5000.0 | . 2 0 | 0 | 1 | 32549 | 3 | 7000.0 | . 3 0 | 1 | 1 | 11179 | 5 | 22000.0 | . 4 1 | 0 | 0 | 213095 | 4 | 3500.0 | . 5 0 | 0 | 1 | 99213 | 4 | 4500.0 | . 6 0 | 1 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 1 | 0 | 54738 | 4 | 7000.0 | . 8 0 | 0 | 1 | 60000 | 4 | 6250.0 | . 9 1 | 0 | 0 | 31600 | 4 | 9700.0 | . Mapping: . Toyota: 001 | Honda: 010 | BMW: 011 | Nissan: 100 | . Frequency Encdoing . encoding for different sizes of test batch might be different. You should think about it beforehand and make preprocessing of the train as close to the test as possible. | . Nevertheless, Frequency Encoding and RFE are especially efficient when your categorical column has “long tails”, i.e. several frequent values and the remaining ones have only a few examples in the dataset. In such a case, Frequency Encoding would catch the similarity between rare columns. | . frequency_encoded_data = data.copy() freq = data.groupby(&#39;Make&#39;).size() frequency_encoded_data[&#39;Make&#39;] = data[&#39;Make&#39;].map(freq) frequency_encoded_data . Make Odometer (KM) Doors Price . 0 4 | 150043 | 4 | 4000.0 | . 1 3 | 87899 | 4 | 5000.0 | . 2 4 | 32549 | 3 | 7000.0 | . 3 1 | 11179 | 5 | 22000.0 | . 4 2 | 213095 | 4 | 3500.0 | . 5 4 | 99213 | 4 | 4500.0 | . 6 3 | 45698 | 4 | 7500.0 | . 7 3 | 54738 | 4 | 7000.0 | . 8 4 | 60000 | 4 | 6250.0 | . 9 2 | 31600 | 4 | 9700.0 | . Mean Encoding . Also Known as Target Encoding, is the go-to categorical encoding used in Kaggle competitions. | . It can figure out the categories that can have simillar effect on predict the target value. | . It doesn&#39;t affect the volume of the data, thus a faster learning process. . | The encoded category values are calculated according to the following formulas: . | . mdl — min data (samples) in leaf, a — smoothing parameter, representing the power of regularization. Recommended values for mdl and a are in the range of 1 to 100. | . It has a huge disadvantage — target leakage: it uses information about the target. Because of the target leakage, model overfits the training data which results in unreliable validation and lower test scores. | . To reduce the effect of target leakage, we may increase regularization (it’s hard to tune those hyperparameters without unreliable validation), | add random noise to the representation of the category in train dataset (some sort of augmentation), or | use Double Validation. | | . mean_encoding = ce.TargetEncoder(cols=&#39;Make&#39;, smoothing=0) mean_encoded_data = mean_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) mean_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] mean_encoded_data . Make Odometer (KM) Doors Price . 0 5437.5 | 150043 | 4 | 4000.0 | . 1 6500.0 | 87899 | 4 | 5000.0 | . 2 5437.5 | 32549 | 3 | 7000.0 | . 3 7645.0 | 11179 | 5 | 22000.0 | . 4 6600.0 | 213095 | 4 | 3500.0 | . 5 5437.5 | 99213 | 4 | 4500.0 | . 6 6500.0 | 45698 | 4 | 7500.0 | . 7 6500.0 | 54738 | 4 | 7000.0 | . 8 5437.5 | 60000 | 4 | 6250.0 | . 9 6600.0 | 31600 | 4 | 9700.0 | . M-estimator Encoding . M-Estimate Encoder is a simplified version of Target Encoder. It has only one hyperparameter — m, which represents the power of regularization | . . In different sources, you may find another formula of M-Estimator. Instead of y+ there is n in the denominator. I found that such representation has similar scores. | . m_estimator_encoding = ce.MEstimateEncoder(cols=&#39;Make&#39;, m=1) m_estimator_encoded_data = m_estimator_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) m_estimator_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] m_estimator_encoded_data . Make Odometer (KM) Doors Price . 0 5879.000000 | 150043 | 4 | 4000.0 | . 1 6786.250000 | 87899 | 4 | 5000.0 | . 2 5879.000000 | 32549 | 3 | 7000.0 | . 3 14822.500000 | 11179 | 5 | 22000.0 | . 4 6948.333333 | 213095 | 4 | 3500.0 | . 5 5879.000000 | 99213 | 4 | 4500.0 | . 6 6786.250000 | 45698 | 4 | 7500.0 | . 7 6786.250000 | 54738 | 4 | 7000.0 | . 8 5879.000000 | 60000 | 4 | 6250.0 | . 9 6948.333333 | 31600 | 4 | 9700.0 | . Weight of Evidence Encoding . Probability Ratio Encoding . both are very similar, and both are explanied in this article. . WOE and Probability Ratio both works with binary classification problems. . i won&#39;t go over them because i can&#39;t grasp it completly. even though i understand how the code works and the instructions of the algorithm. i can&#39;t completly understand how it can affect the learning process. . Weight of Evidence Encdoing . is a commonly used target-based encoder in credit scoring. | . It is a measure of the “strength” of a grouping for separating good and bad risk. | . it might lead to target leakage and overfit. To avoid that, regularization parameter a is induced and WoE is calculated in the following way | . . if is_binary_classification: woe_encoding = ce.WOEEncoder(cols=&#39;Make&#39;) woe_encoded_data = woe_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) woe_encoded_data . Hashing . With Hashing, the number of dimensions will be far less than the number of dimensions with encoding like One Hot Encoding. This method is advantageous when the cardinality of categorical is very high. . hashing_encoding = ce.HashingEncoder(cols=&#39;Make&#39;) hashing_encoded_data = hashing_encoding.fit_transform(data) hashing_encoded_data . col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 Odometer (KM) Doors Price . 0 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 150043 | 4 | 4000.0 | . 1 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 87899 | 4 | 5000.0 | . 2 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 32549 | 3 | 7000.0 | . 3 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11179 | 5 | 22000.0 | . 4 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 213095 | 4 | 3500.0 | . 5 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 99213 | 4 | 4500.0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 54738 | 4 | 7000.0 | . 8 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 60000 | 4 | 6250.0 | . 9 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 31600 | 4 | 9700.0 | . Backward Difference Encdoing . This Tequnique falls under the contrast coding system for categorical features. | It creates N-1 columns (N is no. of columns) | It compares the mean of the dependent variable with the mean of the dependent variable for the prior level | . bd_encoding = ce.BackwardDifferenceEncoder(cols=&#39;Make&#39;, drop_invariant=True) bd_encoded_data = bd_encoding.fit_transform(data) bd_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 -0.75 | -0.5 | -0.25 | 150043 | 4 | 4000.0 | . 1 0.25 | -0.5 | -0.25 | 87899 | 4 | 5000.0 | . 2 -0.75 | -0.5 | -0.25 | 32549 | 3 | 7000.0 | . 3 0.25 | 0.5 | -0.25 | 11179 | 5 | 22000.0 | . 4 0.25 | 0.5 | 0.75 | 213095 | 4 | 3500.0 | . 5 -0.75 | -0.5 | -0.25 | 99213 | 4 | 4500.0 | . 6 0.25 | -0.5 | -0.25 | 45698 | 4 | 7500.0 | . 7 0.25 | -0.5 | -0.25 | 54738 | 4 | 7000.0 | . 8 -0.75 | -0.5 | -0.25 | 60000 | 4 | 6250.0 | . 9 0.25 | 0.5 | 0.75 | 31600 | 4 | 9700.0 | . Looks very similar to Helmert Encoding . bd_encoding.mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: Make_0 Make_1 Make_2 1 -0.75 -0.5 -0.25 2 0.25 -0.5 -0.25 3 0.25 0.5 -0.25 4 0.25 0.5 0.75 -1 0.00 0.0 0.00 -2 0.00 0.0 0.00}] . Leave One Out Encoding . This is very similar to target encoding but excludes the current row’s target when calculating the mean target for a level to reduce outliers. . loo_encoding = ce.LeaveOneOutEncoder(cols=&#39;Make&#39;) loo_encoded_data = loo_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) loo_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] loo_encoded_data . Make Odometer (KM) Doors Price . 0 5916.666667 | 150043 | 4 | 4000.0 | . 1 7250.000000 | 87899 | 4 | 5000.0 | . 2 4916.666667 | 32549 | 3 | 7000.0 | . 3 7645.000000 | 11179 | 5 | 22000.0 | . 4 9700.000000 | 213095 | 4 | 3500.0 | . 5 5750.000000 | 99213 | 4 | 4500.0 | . 6 6000.000000 | 45698 | 4 | 7500.0 | . 7 6250.000000 | 54738 | 4 | 7000.0 | . 8 5166.666667 | 60000 | 4 | 6250.0 | . 9 3500.000000 | 31600 | 4 | 9700.0 | . James-Stein Encoding . Target-based Encoder . | Returns a weighted Average of: . The mean target value for the observed feature value. | The mean for the whole target value | . | It tends to shrinks the average toard the overall average. . | One Major issue, it was defined only for normal distributions . | . if is_normally_distributed: james_stein_encoding = ce.JamesSteinEncoder(cols=[&#39;Make&#39;]) james_stein_encoded_data = james_stein_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) james_stein_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] james_stein_encoded_data . Which encoding method to use for your dataset? . It&#39;s recommended to follow the following chart to figure out which one best suited for your dataset. . You can test on multiple methods on a sample of your dataset, it should give you a good view of which one is better for your situation. . . from sklearn.linear_model import LinearRegression results = {} encoded_datasets = { &quot;One Hot Encoding&quot;: one_hot_encoded_data, &quot;Label Encoding&quot;: label_encoded_data, &quot;Ordinal Encoding&quot;: ordinal_encoded_data, &quot;Helmert Encoding&quot;: helmert_encoded_data, &quot;Binary Encoding&quot;: binary_encoded_data, &quot;Frequency Encoding&quot;: frequency_encoded_data, &quot;Mean Encoding&quot;: mean_encoded_data, &quot;Hashing&quot;: hashing_encoded_data, &quot;Backward Difference Encoding&quot;: bd_encoded_data, &quot;Leave-One-Out&quot;: loo_encoded_data, &quot;M-Estimator Encoding&quot;: m_estimator_encoded_data, } if is_normally_distributed: encoded_datasets[&quot;James-Stein Encoding&quot;] = james_stein_encoded_data if is_binary_classification: encoded_datasets[&quot;Weight of Evidence Encoding&quot;] = woe_encoded_data for encoding_name, encoded_dataset in encoded_datasets.items(): # Split data X = encoded_dataset.drop(&#39;Price&#39;, axis=1) y = encoded_dataset[&#39;Price&#39;] model = LinearRegression() model.fit(X, y) score = model.score(X, y) results[encoding_name] = score; . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . sorted(results.items(), key=lambda x:x[1], reverse=True) . [(&#39;One Hot Encoding&#39;, 0.9945536929215048), (&#39;Helmert Encoding&#39;, 0.9945536929215048), (&#39;Binary Encoding&#39;, 0.9945536929215048), (&#39;Hashing&#39;, 0.9945536929215048), (&#39;Backward Difference Encoding&#39;, 0.9945536929215048), (&#39;M-Estimator Encoding&#39;, 0.9801482890347663), (&#39;Frequency Encoding&#39;, 0.8508034307880376), (&#39;Leave-One-Out&#39;, 0.8166612326802211), (&#39;Mean Encoding&#39;, 0.813764093905115), (&#39;Label Encoding&#39;, 0.7735336830068426), (&#39;Ordinal Encoding&#39;, 0.7735336830068424)] . Notes . Encoders-wise . LE and OE are bad with Linear Models, KNN, and Neural Nets . | OHE is good for tree-based algorithms(and for regression if it produces N-1 columns instead of N) . | Helmert Encoding and Sum Encoding is good for Linear models . | . Algorithm-wise . With Regression, SVM, NN, and clusting algorithms, it&#39;s better to use an encoder that generates N-1 columns | . Resources . Benchmarking Categorical Encoders . All About Categorical Variable Encoding . To-Read . Coding Categorical Variables (Research Paper) .",
            "url": "https://o-hatem.github.io/My-Journey-/jupyter/2021/11/16/Categorical_Encoding.html",
            "relUrl": "/jupyter/2021/11/16/Categorical_Encoding.html",
            "date": " • Nov 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Stuff to Learn",
            "content": "Python Library . datatable. a python library that help loads files into notebooks faster than doing it normally with pd.read_csv(). This a link to sets, each represent a topic on the library. . | gc. Python’s garbage collector module. . | category_encoders. More specific a coder named TargetEncoder . | Useful Concepts . Memory Reduction. For large datasets, Memor reduction is very useful in saving processing time. Found in this notebook . | Using manual cross validation functions. . | Machine Learning Algorithms . XGBoost. |",
            "url": "https://o-hatem.github.io/My-Journey-/markdown/2021/10/27/NewStuffToLearn.html",
            "relUrl": "/markdown/2021/10/27/NewStuffToLearn.html",
            "date": " • Oct 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "My Roadmap for Becoming a Data Scientist",
            "content": "RoadMap . Data Science and Machine Learning Udemy course. (finished in about 3 months) . Just today (Oct 25th, 2021), i almost the couse. It was very since and taught me how to write code for machine learning models with a small transefer learning model in the end. I learned how to : . Preprocess Data (Cleaning, Imputing, etc) | Use Numpy, Pandas for accessing and manipulating data in dataframes | Use Matplotlib to visualize data and figure out how my model is performing | Write an end-to-end machine learning models for regression and classification problems. | Use TensorFlow and TensorFlow Hub with Keras to build a transefer learning model | . but the course lackes the theortical part of machine learning and deep learning which is what i am looking to do next. . | Mathematics for Machine Leaning Coursea course . | What i’m looking to do next . Andrew Ng Youtube playlist . Data Science from Scratch Book from O’reilly . Notes . I should not stop doing Kaggle comeptitions and should be coming up with my own ideas while taking a therotical course. I’ll feel discouraged to compelete the course if all i see is the math and equations. And to not forget the practical part. | . Storytelling . (Oct 27th, 2021) After a little bit of thinking i decided to start with Andrew Ng’s Machine Learning Playlist on YouTube. But before that, i’d like to review all the math needed to complete the course which include: Linear Algebra | Multi-variable calculas | Probability and Statistics | . The Mathematics for Machine Learning Coursera course covers the first two points plus PCA (principal component analysis). so after finishing it, i’ll search for a convinent course for probability and statistics for machine learning. . | . Deadlines . Finish the ZTM:Data Science and Machine Learning course Start on: (idk exactly but around August) | Deadline on: Nov 25th, 2021 | Finished on: Oct 25th, 2021 (one month early) | . | Mathematics for Machine Learning: Linear Algebra (~18h course) Start on: Oct 27th, 2021 | Deadline on: Nov 27th, 2021 | Finished on: TBC | . | .",
            "url": "https://o-hatem.github.io/My-Journey-/markdown/2021/10/25/RoadMap.html",
            "relUrl": "/markdown/2021/10/25/RoadMap.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "ML Questions",
            "content": "ML-related Questions . What is Cross Validation and what is it used for? . Cross Validation is a statstical way to predict the accuracy of a machine learning model. It helps reduce the change of overfitting to the data and selection bias. it’s used specially with small datasets where each data point may carry an important piece of information to help the machine learning model better predict. . Types of Cross Validation: . Classification has two main broad types: . non-Exhasutive Methods. non-Exhaustive method doesn’t compute all the ways of splitting data. . Holdout method. this method word by dividing the data into two sets, training and testing sets. Usually the percentage is 80:20 or 75:25 for training and testing respcetivly. . | K fold cross validation. it’s an improvment to the holdout method. it works by dividing the data into k folds, where training is performed on k-1 folds and testing is performed on 1 fold. then it chooses a different fold for testing while the rest for training, and so for till all folds are used as a testing fold once. Finally, it takes the average of all combination. . | Stratified K fold cross validation. There maybe a problem when using k fold CV when working on a classifcation problem. Where one fold can have a majority of a class resulting in selection bias for a particular fold. to prevent that stratified k CV is used to ensure that every fold consists balanced data between the class to avoid any bias in the data. . | | Exhaustive Methods. these are methods where is split the data into every possible combination of training and test sets. . Leave-P-out Method. it works by having p data points as the test set and (n-p) as the training set. where p will be every combination of the data points in the original dataset. The higher the p, the more combinations there are, the more processing power needed. . | Leave-one-out method. it is a variance of the Leave-p-out method where p is equal to 1. it is much less exhaustive where the number of combinations is euqal to n. . | | Rolling Cross validation. Rolling cross validation is used for time series datasets. It makes sure the golden rule of time series probelms (“look at the past to predict the future”) is not broken. it divides the data by a time frame and then train each split sequentially. e.g. . | Training set: [1], test set: [2] Training set: [1, 2], test set: [3] Training set: [1, 2, 3], test set: [4] . . Training set: [1, 2, 3, ..., n-1], test set: [n] .",
            "url": "https://o-hatem.github.io/My-Journey-/markdown/2021/10/25/MLQuestions.html",
            "relUrl": "/markdown/2021/10/25/MLQuestions.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Good Reading Articles",
            "content": "Good Reading Articles . Advice for Better Blog Posts By Rachel Thomas It’s a great article for begginers on how and why to start blogging. I liked how her words makes me feel encourged to do so. There are some great tips that i found very helpful. There are some points that i feel the need to keep in my mind all the time while blogging and i’ll do my best to do so. . | How to Learn Mathematics For Machine Learning? What Concepts do You Need to Master in Data Science? A really good article on all the math a data science or machine learning learner will need. It covers the required math branches you’ll need to study as well as the most important topics in each of them. And at the end, it got some good reccomendation for resources to learn the necessary math. . | .",
            "url": "https://o-hatem.github.io/My-Journey-/markdown/2021/10/25/GoodReading.html",
            "relUrl": "/markdown/2021/10/25/GoodReading.html",
            "date": " • Oct 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://o-hatem.github.io/My-Journey-/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://o-hatem.github.io/My-Journey-/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}