{
  
    
        "post0": {
            "title": "EDA Champions League Finals",
            "content": "import requests from bs4 import BeautifulSoup import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import re %matplotlib inline . Retrieving Data . Data collected from Wikipedia&#39;s Champions league finals&#39; page . html = requests.get(&quot;https://en.wikipedia.org/wiki/List_of_UEFA_Champions_League_finals&quot;) soup = BeautifulSoup(html.content) . finals_table = soup.select(&quot;table.wikitable.plainrowheaders.sortable&quot;)[0] finals_body = finals_table.tbody . matches = finals_body.find_all(&#39;tr&#39;, class_=&#39;&#39;) . matches.pop(0) matches.pop(0) . &lt;tr&gt; &lt;th rowspan=&#34;2&#34; scope=&#34;col&#34;&gt;Season &lt;/th&gt; &lt;th colspan=&#34;2&#34; scope=&#34;col&#34;&gt;Winners &lt;/th&gt; &lt;th rowspan=&#34;2&#34; scope=&#34;col&#34;&gt;Score &lt;/th&gt; &lt;th colspan=&#34;2&#34; scope=&#34;col&#34;&gt;Runners-up &lt;/th&gt; &lt;th rowspan=&#34;2&#34; scope=&#34;col&#34;&gt;Venue &lt;/th&gt; &lt;th rowspan=&#34;2&#34; scope=&#34;col&#34;&gt;Attend­ance&lt;sup class=&#34;reference&#34; id=&#34;cite_ref-12&#34;&gt;&lt;a href=&#34;#cite_note-12&#34;&gt;[12]&lt;/a&gt;&lt;/sup&gt; &lt;/th&gt;&lt;/tr&gt; . &lt;tr&gt; &lt;th scope=&#34;col&#34;&gt;Nation &lt;/th&gt; &lt;th scope=&#34;col&#34;&gt;Team &lt;/th&gt; &lt;th scope=&#34;col&#34;&gt;Nation &lt;/th&gt; &lt;th scope=&#34;col&#34;&gt;Team &lt;/th&gt;&lt;/tr&gt; . # since it&#39;s just a score for a replayed match. will be edited in the dataframe matches.pop(19) . &lt;tr&gt; &lt;td align=&#34;center&#34; bgcolor=&#34;d1f7a5&#34;&gt;&lt;a href=&#34;/wiki/1974_European_Cup_Final&#34; title=&#34;1974 European Cup Final&#34;&gt;4–0&lt;/a&gt;&lt;sup&gt;&amp;amp;&lt;/sup&gt; &lt;/td&gt; &lt;td align=&#34;center&#34;&gt;23,325 &lt;/td&gt;&lt;/tr&gt; . def get_matches(matches): df = pd.DataFrame(columns=[&#39;season&#39;, &#39;winner_nation&#39;, &#39;winner_team&#39;, &#39;score&#39;, &#39;runnerup_nation&#39;, &#39;runnerup_team&#39;, &#39;venue&#39;, &#39;attendence&#39;]) for match in matches: th = match.find_next(&#39;th&#39;) tds = match.find_all(&#39;td&#39;) season = th.a.text winner_nationality = tds[0].span.a.text winner_name = tds[1].a.text # how the match ended try: if tds[2][&#39;bgcolor&#39;] == &#39;FBCEB1&#39;: finished = &#39;extra time&#39; elif tds[2][&#39;bgcolor&#39;] == &#39;cedff2&#39;: finished = &#39;penalties&#39; else: finished = &#39;normal&#39; except: finished = &#39;normal&#39; score = tds[2].a.text runnerup_nationality = tds[3].span.a.text runnerup_name = tds[4].a.text venue = tds[5].text attendance = tds[6].text df = df.append({&#39;season&#39;:season, &#39;winner_nation&#39;: winner_nationality, &#39;winner_team&#39;:winner_name, &#39;score&#39;: score, &#39;runnerup_nation&#39;: runnerup_nationality, &#39;runnerup_team&#39;: runnerup_name, &#39;venue&#39;: venue, &#39;attendence&#39;: attendance, &#39;finished&#39;: finished}, ignore_index=True) return df . champions_finals = get_matches(matches) . champions_finals.head() . season winner_nation winner_team score runnerup_nation runnerup_team venue attendence finished . 0 1955–56 | ESP | Real Madrid | 4–3 | FRA | Reims | Parc des Princes, Paris, France n | 38,239 n | normal | . 1 1956–57 | ESP | Real Madrid | 2–0 | ITA | Fiorentina | Santiago Bernabéu, Madrid, Spain n | 124,000 n | normal | . 2 1957–58 | ESP | Real Madrid | 3–2 | ITA | Milan | Heysel Stadium, Brussels, Belgium n | 67,000 n | extra time | . 3 1958–59 | ESP | Real Madrid | 2–0 | FRA | Reims | Neckarstadion, Stuttgart, West Germany n | 72,000 n | normal | . 4 1959–60 | ESP | Real Madrid | 7–3 | GER | Eintracht Frankfurt | Hampden Park, Glasgow, Scotland n | 127,621 n | normal | . Cleaning Data . Remove unnecessary ends . in venue and attendance. sol . champions_finals[[&#39;venue&#39;, &#39;attendence&#39;]] = champions_finals[[&#39;venue&#39;, &#39;attendence&#39;]].apply(lambda x: x.str.strip(), ) . champions_finals.tail(3) . season winner_nation winner_team score runnerup_nation runnerup_team venue attendence finished . 64 2019–20 | GER | Bayern Munich | 1–0 | FRA | Paris Saint-Germain | Estádio da Luz, Lisbon, Portugal[l] | 0[m] | normal | . 65 2020–21 | ENG | Chelsea | 1–0 | ENG | Manchester City | Estádio do Dragão, Porto, Portugal[n] | 14,110[o] | normal | . 66 2021–22 | ESP | Real Madrid | 1–0 | ENG | Liverpool | Stade de France, Saint-Denis, France[p] | 75,000 | normal | . pattern = r&#39; [.*? ]&#39; champions_finals[&#39;venue&#39;] = champions_finals[&#39;venue&#39;].apply(lambda x: re.sub(pattern, &#39;&#39;, str(x))) champions_finals[&#39;attendence&#39;] = champions_finals[&#39;attendence&#39;].apply(lambda x: re.sub(pattern, &#39;&#39;, str(x))) . champions_finals.tail(3) . season winner_nation winner_team score runnerup_nation runnerup_team venue attendence finished . 64 2019–20 | GER | Bayern Munich | 1–0 | FRA | Paris Saint-Germain | Estádio da Luz, Lisbon, Portugal | 0 | normal | . 65 2020–21 | ENG | Chelsea | 1–0 | ENG | Manchester City | Estádio do Dragão, Porto, Portugal | 14,110 | normal | . 66 2021–22 | ESP | Real Madrid | 1–0 | ENG | Liverpool | Stade de France, Saint-Denis, France | 75,000 | normal | . split the score into two columns . split into winner_score and runnerup_score. sol. . then merge them into the dataframe sol . scores = pd.DataFrame(champions_finals[&#39;score&#39;].str.split(&#39;–&#39;).tolist(), columns=[&#39;winner_score&#39;, &#39;runnerup_score&#39;]) champions_finals = champions_finals.join(scores).drop(columns=[&#39;score&#39;], axis=1) . champions_finals.head(3) . season winner_nation winner_team runnerup_nation runnerup_team venue attendence finished winner_score runnerup_score . 0 1955–56 | ESP | Real Madrid | FRA | Reims | Parc des Princes, Paris, France | 38,239 | normal | 4 | 3 | . 1 1956–57 | ESP | Real Madrid | ITA | Fiorentina | Santiago Bernabéu, Madrid, Spain | 124,000 | normal | 2 | 0 | . 2 1957–58 | ESP | Real Madrid | ITA | Milan | Heysel Stadium, Brussels, Belgium | 67,000 | extra time | 3 | 2 | . Convert columns into numerical . champions_finals.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 67 entries, 0 to 66 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 season 67 non-null object 1 winner_nation 67 non-null object 2 winner_team 67 non-null object 3 runnerup_nation 67 non-null object 4 runnerup_team 67 non-null object 5 venue 67 non-null object 6 attendence 67 non-null object 7 finished 67 non-null object 8 winner_score 67 non-null object 9 runnerup_score 67 non-null object dtypes: object(10) memory usage: 5.4+ KB . attendence, winner_score, and runnerup_score need to be changed into numerical. But first we need to clean attendance column . champions_finals[&#39;attendence&#39;] = champions_finals[&#39;attendence&#39;].str.replace(&#39;,&#39;, &#39;&#39;) . champions_finals.head(3) . season winner_nation winner_team runnerup_nation runnerup_team venue attendence finished winner_score runnerup_score . 0 1955–56 | ESP | Real Madrid | FRA | Reims | Parc des Princes, Paris, France | 38239 | normal | 4 | 3 | . 1 1956–57 | ESP | Real Madrid | ITA | Fiorentina | Santiago Bernabéu, Madrid, Spain | 124000 | normal | 2 | 0 | . 2 1957–58 | ESP | Real Madrid | ITA | Milan | Heysel Stadium, Brussels, Belgium | 67000 | extra time | 3 | 2 | . champions_finals[&#39;attendence&#39;] = champions_finals[&#39;attendence&#39;].astype(&#39;int&#39;) champions_finals[&#39;winner_score&#39;] = champions_finals[&#39;winner_score&#39;].astype(&#39;int&#39;) champions_finals[&#39;runnerup_score&#39;] = champions_finals[&#39;runnerup_score&#39;].astype(&#39;int&#39;) . champions_finals.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 67 entries, 0 to 66 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 season 67 non-null object 1 winner_nation 67 non-null object 2 winner_team 67 non-null object 3 runnerup_nation 67 non-null object 4 runnerup_team 67 non-null object 5 venue 67 non-null object 6 attendence 67 non-null int64 7 finished 67 non-null object 8 winner_score 67 non-null int64 9 runnerup_score 67 non-null int64 dtypes: int64(3), object(7) memory usage: 5.4+ KB . Insights . def barplotting(x, y, xticksrange=False, yticksrange=False, xticksstep=1, yticksstep=1, figsize=(8, 5), style=&#39;whitegrid&#39;): plt.figure(figsize=figsize) sns.set_style(style) if xticksrange: plt.xticks(np.arange(0, xticksrange+1, xticksstep)) if yticksrange: plt.yticks(np.arange(0, yticksrange+1, yticksstep)) sns.barplot(x=x, y=y, palette=&#39;Set1&#39;); . Q1: Most winning teams . most_winning_teams = champions_finals[&#39;winner_team&#39;].value_counts() barplotting(x=most_winning_teams, y=most_winning_teams.index, xticksrange=max(most_winning_teams)) . Q2: Most Runner up teams . most_running_up = champions_finals[&#39;runnerup_team&#39;].value_counts() barplotting(x=most_running_up, y=most_running_up.index, xticksrange=max(most_running_up), figsize=(12,8)) . Q3: What is the most common scores? . finals_score = champions_finals.value_counts(subset=[&#39;winner_score&#39;, &#39;runnerup_score&#39;]) finals_score.index = finals_score.index.map(lambda x: str(x[0]) + &#39;-&#39; + str(x[1])) . plt.pie(finals_score, labels=finals_score.index, autopct=&#39;%.1f%%&#39;, startangle=60, radius=2.5); . Q4: How games ended . finished = champions_finals[&#39;finished&#39;].value_counts() barplotting(x=finished.index, y=finished, xticksrange=max(finished)) . Q5) Most playing nations . Most winning . most_wining_nations = champions_finals[&#39;winner_nation&#39;].value_counts() barplotting(x=most_wining_nations.index, y=most_wining_nations, yticksrange=max(most_wining_nations)) . Most running up . most_runningup_nations = champions_finals[&#39;runnerup_nation&#39;].value_counts() barplotting(x=most_runningup_nations.index, y=most_runningup_nations, yticksrange=max(most_wining_nations)) . Appearances . participation = champions_finals[&#39;winner_nation&#39;].append(champions_finals[&#39;runnerup_nation&#39;], ignore_index=True) participation = participation.value_counts() barplotting(x=participation.index, y=participation, yticksrange=max(participation), yticksstep=4) . Q6: Most used Stadium in finals . most_used_stadiums = champions_finals[&#39;venue&#39;].value_counts() barplotting(x=most_used_stadiums, y=most_used_stadiums.index, yticksrange=max(most_used_stadiums), figsize=(12, 10)) . Q7: Average attendance in finals . line or dot at the mean of the graph. sol . attendance_mean = np.mean(champions_finals[&#39;attendence&#39;]) plt.figure(figsize=(12,7)) sns.histplot(x=&#39;attendence&#39;, data=champions_finals) plt.xticks(np.arange(0, 200_000, 10_000), rotation=90) plt.xlim([0, 200000]); . Q8: Most conceded goals and scored goals . total_goals = pd.DataFrame(columns=[&#39;team&#39;, &#39;scored&#39;, &#39;conceded&#39;]) unique_teams = champions_finals[&#39;winner_team&#39;].append(champions_finals[&#39;runnerup_team&#39;], ignore_index=True).unique() . for team in unique_teams: winning = champions_finals[champions_finals[&#39;winner_team&#39;] == team][[&#39;winner_score&#39;, &#39;runnerup_score&#39;]].sum() runningup = champions_finals[champions_finals[&#39;runnerup_team&#39;] == team][[&#39;winner_score&#39;, &#39;runnerup_score&#39;]].sum() total_scored = winning[0] + runningup[1] total_conceded = winning[1] + runningup[0] total_goals = total_goals.append({&#39;team&#39;: team, &#39;scored&#39;: int(total_scored), &#39;conceded&#39;: int(total_conceded)}, ignore_index=True) . scoring_sorted = total_goals.sort_values(by=&#39;scored&#39;, ascending=False) barplotting(x=scoring_sorted.team[:10], y=scoring_sorted.scored[:10], figsize=(10, 5)) plt.title(&#39;Most Scoring teams&#39;) plt.xticks(rotation=60); . conceding_sorted = total_goals.sort_values(by=&#39;conceded&#39;, ascending=False) barplotting(x=conceding_sorted.team[:10], y=conceding_sorted.conceded[:10], figsize=(10, 5)) plt.title(&#39;Most Conceding teams&#39;) plt.xticks(rotation=60); . df = scoring_sorted[:10].melt(id_vars=&#39;team&#39;, var_name=&#39;kind&#39;, value_name=&#39;goals&#39;) sns.catplot(x=&#39;team&#39;, y=&#39;goals&#39;, hue=&#39;kind&#39;, data=df, kind=&#39;bar&#39;, height=8, aspect=1.5) plt.title(&#39;Scored and Conceded goals for Highest Scoring Teams&#39;) plt.xlabel(&#39;Teams&#39;) plt.ylabel(&#39;Goals&#39;) plt.xticks(rotation=60) plt.yticks(np.arange(0, 50, 2)); .",
            "url": "https://o-hatem.github.io/My-Journey/web%20scraping/eda/visualization/2022/06/09/Champions-league-finals.html",
            "relUrl": "/web%20scraping/eda/visualization/2022/06/09/Champions-league-finals.html",
            "date": " • Jun 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "EDA Movie Industry in Egypt",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import requests import json %matplotlib inline . pd.set_option(&#39;max_columns&#39;, None) . Challenge 1: can&#39;t open the JSON file correctly sol . with open(&#39;../data/movies.json&#39;,&#39;r&#39;) as f: data = json.loads(f.read()) # Flatten data movies = pd.json_normalize(data, record_path =[&#39;results&#39;]) . with open(&#39;../data/series.json&#39;, &#39;r&#39;) as f: data = json.loads(f.read()) series = pd.json_normalize(data, record_path=[&#39;results&#39;]) . Data Preprocessing . Formatting Released Date . For series . series[&#39;description&#39;] = series[&#39;description&#39;].apply(lambda x: x.strip(&#39;()&#39;).split(&#39;–&#39;)[0]) . series[~series[&#39;description&#39;].str.isnumeric()][&#39;description&#39;] . 47 I) (2021 110 I) (2021 162 235 II) (2021 240 Name: description, dtype: object . series.loc[47, &#39;description&#39;] = &#39;2021&#39; series.loc[110, &#39;description&#39;] = &#39;2021&#39; series.loc[235, &#39;description&#39;] = &#39;2021&#39; . series[~series[&#39;description&#39;].str.isnumeric()][&#39;description&#39;] . 162 240 Name: description, dtype: object . indexes = series[~series[&#39;description&#39;].str.isnumeric()].index series.drop(indexes, axis=0, inplace=True) . For movies . movies[&#39;description&#39;] = movies[&#39;description&#39;].apply(lambda x: x.strip(&#39;()&#39;).split()[0]) . movies[~movies[&#39;description&#39;].str.isnumeric()][&#39;description&#39;] . 23 TV 33 TV Name: description, dtype: object . indexes = movies[~movies[&#39;description&#39;].str.isnumeric()].index movies.drop(indexes, axis=0, inplace=True) . movies[~movies[&#39;description&#39;].str.isnumeric()][&#39;description&#39;] . Series([], Name: description, dtype: object) . Adding type column . movies[&#39;type&#39;] = &#39;Movie&#39; series[&#39;type&#39;] = &#39;Series&#39; . Appending the two series . works = series.append(movies, ignore_index=True) . Drop unnecessary columns . works.drop([&#39;runtimeStr&#39;, &#39;contentRating&#39;, &#39;metacriticRating&#39;, &#39;plot&#39;, &#39;stars&#39;, &#39;starList&#39;], axis=1, inplace=True) . Changing the numerical datatypes . indexes = works[works[&#39;imDbRatingVotes&#39;].isnull()].index works.loc[indexes, &#39;imDbRatingVotes&#39;] = 0 . works[&#39;imDbRatingVotes&#39;] = works[&#39;imDbRatingVotes&#39;].astype(&#39;int&#39;) works[&#39;imDbRating&#39;] = works[&#39;imDbRating&#39;].astype(&#39;float&#39;) works[&#39;description&#39;] = works[&#39;description&#39;].astype(&#39;int&#39;) . Separate the genres . Challenge 2: can&#39;t separate the genres. sol . works[works[&#39;genres&#39;].isnull()] . id image title description genres genreList imDbRating imDbRatingVotes type . 282 tt3253984 | https://imdb-api.com/images/original/nopicture... | Howa Al Naharda Aih | 2007 | None | None | NaN | 0 | Movie | . 289 tt3254058 | https://imdb-api.com/images/original/nopicture... | Tarh El Sabbar | 2006 | None | None | NaN | 0 | Movie | . indexes = works[works[&#39;genres&#39;].isnull()].index works.drop(indexes, inplace=True) works.reset_index(drop=True, inplace=True) . works[&#39;no_of_genres&#39;] = works[&#39;genres&#39;].apply(lambda x: len(x.split(&#39;, &#39;))) . np.max(works[&#39;no_of_genres&#39;]) . 3 . works[&#39;genres&#39;][works[&#39;no_of_genres&#39;] == 1].value_counts() . Drama 82 Comedy 41 Action 8 Documentary 8 Thriller 7 Musical 3 Talk-Show 2 Crime 2 Romance 2 Horror 2 History 2 Mystery 2 Fantasy 1 Animation 1 Adventure 1 Biography 1 Music 1 Name: genres, dtype: int64 . unique_genres = [] for genre_list in works[&#39;genres&#39;].str.strip(&#39;][&#39;).str.split(&#39;, &#39;): for genre in genre_list: if genre not in unique_genres: unique_genres.append(genre) print(unique_genres) . [&#39;Drama&#39;, &#39;Fantasy&#39;, &#39;Horror&#39;, &#39;Comedy&#39;, &#39;Romance&#39;, &#39;Action&#39;, &#39;Biography&#39;, &#39;Thriller&#39;, &#39;History&#39;, &#39;War&#39;, &#39;Mystery&#39;, &#39;Adventure&#39;, &#39;Crime&#39;, &#39;Sci-Fi&#39;, &#39;Family&#39;, &#39;Music&#39;, &#39;Musical&#39;, &#39;Talk-Show&#39;, &#39;Reality-TV&#39;, &#39;Animation&#39;, &#39;Short&#39;, &#39;Documentary&#39;] . len(unique_genres) . 22 . works.shape . (297, 10) . for unique_genre in unique_genres: works[unique_genre] = 0 . works.shape . (297, 32) . for index, genre_list in enumerate(works[&#39;genres&#39;].str.strip(&#39;][&#39;).str.split(&#39;, &#39;)): for genre in genre_list: works.loc[index, genre] = 1 . works.head(2) . id image title description genres genreList imDbRating imDbRatingVotes type no_of_genres Drama Fantasy Horror Comedy Romance Action Biography Thriller History War Mystery Adventure Crime Sci-Fi Family Music Musical Talk-Show Reality-TV Animation Short Documentary . 0 tt19495202 | https://imdb-api.com/images/original/MV5BNDNjN... | Suits | 2022 | Drama | [{&#39;key&#39;: &#39;Drama&#39;, &#39;value&#39;: &#39;Drama&#39;}] | 7.4 | 179 | Series | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 tt12411074 | https://imdb-api.com/images/original/MV5BNGY1Y... | Paranormal | 2020 | Drama, Fantasy, Horror | [{&#39;key&#39;: &#39;Drama&#39;, &#39;value&#39;: &#39;Drama&#39;}, {&#39;key&#39;: &#39;... | 8.0 | 71528 | Series | 3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . works.drop([&#39;genres&#39;, &#39;genreList&#39;], axis=1, inplace=True) . Answering Questions . Q1) how many works/series have been made in each genre? . summations = [] for genre in unique_genres: summations.append(np.sum(works[genre])) no_works_in_genre = pd.Series(summations, index=unique_genres).sort_values(ascending=False) . plt.figure(figsize=(10, 6)) sns.barplot(x=no_works_in_genre.index, y=no_works_in_genre.values) plt.xticks(rotation=90) plt.xlabel(&#39;Genres&#39;) plt.ylabel(&#39;Number of Works&#39;); . Q2) How many works have been made each year? . works_per_year = works[[&#39;id&#39;, &#39;description&#39;]].groupby(by=&#39;description&#39;, as_index=False).count() plt.figure(figsize=(10, 6)) sns.barplot(x=&#39;description&#39;, y=&#39;id&#39;, data=works_per_year) plt.xticks(rotation=90) plt.xlabel(&#39;Year&#39;) plt.ylabel(&#39;Number of works&#39;); . Q3) What is the average rating for movies and series? . series[&#39;imDbRating&#39;].isnull().sum() movies[&#39;imDbRating&#39;].isnull().sum() . 10 . 34 . works[works[&#39;type&#39;] == &#39;Movie&#39;][&#39;imDbRating&#39;].mean() works[works[&#39;type&#39;] == &#39;Series&#39;][&#39;imDbRating&#39;].mean() works[&#39;imDbRating&#39;].mean() . 6.7176470588235295 . 6.726050420168067 . 6.725490196078432 .",
            "url": "https://o-hatem.github.io/My-Journey/2022/05/25/Movies-and-Series.html",
            "relUrl": "/2022/05/25/Movies-and-Series.html",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Garbage Collection in Python",
            "content": "import datatable as dt import pandas as pd . data = dt.fread(&#39;car-sales.csv&#39;).to_pandas() data.head() . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . 1 Honda | Red | 87899 | 4 | $5,000.00 | . 2 Toyota | Blue | 32549 | 3 | $7,000.00 | . 3 BMW | Black | 11179 | 5 | $22,000.00 | . 4 Nissan | White | 213095 | 4 | $3,500.00 | . data_2 = data . print(&quot;data id&quot;, id(data)) print(&quot;data_2 id&quot;, id(data_2)) . data id 140536166553728 data_2 id 140536166553728 . it&#39;s the same address, so changing in one means change in the other . data[&#39;test_1&#39;] = &#39;test_1&#39; . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . but using the copy() method it creates a variable with a different address . data_3 = data.copy() data_3.head(1) . Make Colour Odometer (KM) Doors Price test_1 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | . print(&quot;data id&quot;, id(data)) print(&quot;data_2 id&quot;, id(data_2)) print(&quot;data_3 id&quot;, id(data_3)) . data id 140536166553728 data_2 id 140536166553728 data_3 id 140534800707936 . data_2[&quot;test_2&quot;] = &quot;test_2&quot; . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_1 test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | test_2 | . Make Colour Odometer (KM) Doors Price test_1 test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_1 | test_2 | . Changing a table inplace means it doesn&#39;t change its address. It works with the same allocated memory . data.drop(&#39;test_1&#39;, axis=1, inplace=True) . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . But assigning it to a new variable means a new memory slot needs to be allocated . data = data.drop(&#39;test_2&#39;, axis=1) . display(data.head(1)) display(data_2.head(1)) . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . Make Colour Odometer (KM) Doors Price test_2 . 0 Toyota | White | 150043 | 4 | $4,000.00 | test_2 | . What if we split the data does it change the address? . X = data.drop(&#39;Price&#39;, axis=1) y = data[&#39;Price&#39;] . print(&quot;X id&quot;, id(X)) print(&quot;y id&quot;, id(y)) print(&quot;data id&quot;, id(data)) . X id 140534800254576 y id 140534800254288 data id 140534800254048 . Trying it with simple variables . a = 1 b = a c = a + 1 print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) a = a+1 print() print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) . a id 140536221767984 b id 140536221767984 c id 140536221768016 a id 140536221768016 b id 140536221767984 c id 140536221768016 . a = 1 b = a c = a + 1 print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) a = b+1 print() print(&quot;a id&quot;, id(a)) print(&quot;b id&quot;, id(b)) print(&quot;c id&quot;, id(c)) . a id 140536221767984 b id 140536221767984 c id 140536221768016 a id 140536221768016 b id 140536221767984 c id 140536221768016 . so any thing in the the rhs will evaluate as a new variable, therefore new memory address . Just noticed that increasing the value of a by 1 (now it&#39;s 2, the same as c) will make the address of a and c the same. . Does that mean Python will create one memory slot for a specific value and any variable with that value will get the same address? . d = 2 print(&quot;d id&quot;, id(d)) . d id 140536221768016 . it looks like it. i&#39;m impressed ngl . i read a little into an article about python garbage collection. . What i got is, yes, it allocate only one memory slot for the object and in the underlying layers it got something called reference count, where it keeps count of all objects uses this memory slot and when the count is equal to 0 it deallocate it immediatly. . import sys sys.getrefcount(2) e = 2 sys.getrefcount(2) del e sys.getrefcount(2) . 1928 . 1929 . 1928 . from this article . After using this type of statement, the objects are no longer accessible for the given code. But, the objects are still there in the memory. . so we use gc.collect() after it to free the memory. . import gc . a = 19 del a gc.collect() . 0 .",
            "url": "https://o-hatem.github.io/My-Journey/2022/01/22/Garbage-collection-in-Python.html",
            "relUrl": "/2022/01/22/Garbage-collection-in-Python.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Categorical Encodings",
            "content": "About . Most Machine Learning algorithms can&#39;t make use of categorical features untill they are converted into numerical values. that&#39;s where &quot;Categorical Encoding&quot; comes into play. There are a lot of different ways to convert categorical features, some are better than other in different situations. I&#39;ll be doing my best to clarify how each categorical encoding work and when to use them. . Categorical Encoding can be divided into two broad categories, Nominal (there&#39;s no order to the categories) and Oridnal (there&#39;s some order into them). . Examples for Nominal: . Red, Blue, Black | Car, Ship, Plane | . Examples for Ordinal: . Excellent, Very Good, Good, Failed | Tall, medium, short | . the most stable and accurate encoders are target-based encoders with Double Validation: Catboost Encoder, James-Stein Encoder, M-estimator Encoder and Target Encoder | . Using Single Validation will result in much better outcome than not using validation at all. Double validation will achieve more stable score but it would costs more resources and time. | . Regularization is a must for target-based encoders. | . Reference y, y+ = # of target values, # of true target variable | n, n+ = # of observations for a given value in a categoricla column | xi, yi = ith value of category and target | a = regularization hyperparameter, default is prior(mean value of the target) | . | . is_binary_classification = False # set to True, if you data is normally distributed (for James-Stein Encoding) is_normally_distributed = False . Data . The data we&#39;ll be using is from ZeroToMastery: Machine Learning and Data Science Udemy Course. . data = pd.read_csv(&#39;https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales.csv&#39;) data . Make Colour Odometer (KM) Doors Price . 0 Toyota | White | 150043 | 4 | $4,000.00 | . 1 Honda | Red | 87899 | 4 | $5,000.00 | . 2 Toyota | Blue | 32549 | 3 | $7,000.00 | . 3 BMW | Black | 11179 | 5 | $22,000.00 | . 4 Nissan | White | 213095 | 4 | $3,500.00 | . 5 Toyota | Green | 99213 | 4 | $4,500.00 | . 6 Honda | Blue | 45698 | 4 | $7,500.00 | . 7 Honda | Blue | 54738 | 4 | $7,000.00 | . 8 Toyota | White | 60000 | 4 | $6,250.00 | . 9 Nissan | White | 31600 | 4 | $9,700.00 | . data[&#39;Price&#39;] = data[&#39;Price&#39;].str.replace(&#39;$&#39;, &#39;&#39;).str.replace(&#39;,&#39;, &#39;&#39;).str.replace(&#39;.&#39;, &#39;&#39;).astype(int)/100 data.drop(&#39;Colour&#39;, axis=1, inplace=True) data . Make Odometer (KM) Doors Price . 0 Toyota | 150043 | 4 | 4000.0 | . 1 Honda | 87899 | 4 | 5000.0 | . 2 Toyota | 32549 | 3 | 7000.0 | . 3 BMW | 11179 | 5 | 22000.0 | . 4 Nissan | 213095 | 4 | 3500.0 | . 5 Toyota | 99213 | 4 | 4500.0 | . 6 Honda | 45698 | 4 | 7500.0 | . 7 Honda | 54738 | 4 | 7000.0 | . 8 Toyota | 60000 | 4 | 6250.0 | . 9 Nissan | 31600 | 4 | 9700.0 | . data[&#39;Make&#39;].value_counts() . Toyota 4 Honda 3 Nissan 2 BMW 1 Name: Make, dtype: int64 . One Hot Encoding . It uses a vector to denote the absence or existence for a category. the length of the vector depends on the number of categories in the feature. | . It creates N columns (N is the number of categories). | . When solving regression problems it&#39;s better to keep the columns at N-1 to ensure the correct number of degrees of freedom (N-1) | . It&#39;s recommended to Use N columns for classifiction, espically when using a tree-based algorithm. | . It&#39;s recommended to use N-1 columns for algorithms that look at all the features simultaneoulsy during training (e.g. SVM, NN, clustring algorithms). | . If the number of categories is big, it will slow the training process. OHE expands the size of your dataset, which makes it memory-inefficient encoder. | . There are several strategies to overcome the memory problem with OHE, one of which is working with sparse not dense data representation.?? | . ohe = ce.OneHotEncoder(cols=&#39;Make&#39;, use_cat_names=True) one_hot_encoded_data = ohe.fit_transform(data) one_hot_encoded_data . Make_Toyota Make_Honda Make_BMW Make_Nissan Odometer (KM) Doors Price . 0 1 | 0 | 0 | 0 | 150043 | 4 | 4000.0 | . 1 0 | 1 | 0 | 0 | 87899 | 4 | 5000.0 | . 2 1 | 0 | 0 | 0 | 32549 | 3 | 7000.0 | . 3 0 | 0 | 1 | 0 | 11179 | 5 | 22000.0 | . 4 0 | 0 | 0 | 1 | 213095 | 4 | 3500.0 | . 5 1 | 0 | 0 | 0 | 99213 | 4 | 4500.0 | . 6 0 | 1 | 0 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 1 | 0 | 0 | 54738 | 4 | 7000.0 | . 8 1 | 0 | 0 | 0 | 60000 | 4 | 6250.0 | . 9 0 | 0 | 0 | 1 | 31600 | 4 | 9700.0 | . Sum Encoding . also known as Deviation encoding or Effect encoding . | Sum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models. . | However, the difference between them is the interpretation of LR coefficients.?? . OHE model the intercept represents the mean for the baseline condition and coefficients represents simple effects (the difference between one particular condition and the baseline), | in Sum Encoder model the intercept represents the grand mean (across all conditions) and the coefficients can be interpreted directly as the main effects. | . | . sum_encoding = ce.SumEncoder(cols=[&#39;Make&#39;]) sum_encoded_data = sum_encoding.fit_transform(data).drop(&#39;intercept&#39;, axis=1) sum_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 1.0 | 0.0 | 0.0 | 150043 | 4 | 4000.0 | . 1 0.0 | 1.0 | 0.0 | 87899 | 4 | 5000.0 | . 2 1.0 | 0.0 | 0.0 | 32549 | 3 | 7000.0 | . 3 0.0 | 0.0 | 1.0 | 11179 | 5 | 22000.0 | . 4 -1.0 | -1.0 | -1.0 | 213095 | 4 | 3500.0 | . 5 1.0 | 0.0 | 0.0 | 99213 | 4 | 4500.0 | . 6 0.0 | 1.0 | 0.0 | 45698 | 4 | 7500.0 | . 7 0.0 | 1.0 | 0.0 | 54738 | 4 | 7000.0 | . 8 1.0 | 0.0 | 0.0 | 60000 | 4 | 6250.0 | . 9 -1.0 | -1.0 | -1.0 | 31600 | 4 | 9700.0 | . Helmert Encoding . Helmert coding is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. | . This type of encoding can be useful in certain situations where levels of the categorical variable are ordered, say, from lowest to highest, or from smallest to largest.The mean of the dependent variable for a level is compared to the mean of the dependent variable over all previous levels. source . | . helmert_encoding = ce.HelmertEncoder(cols=&#39;Make&#39;, drop_invariant=True) helmert_encoded_data = helmert_encoding.fit_transform(data) helmert_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 -1.0 | -1.0 | -1.0 | 150043 | 4 | 4000.0 | . 1 1.0 | -1.0 | -1.0 | 87899 | 4 | 5000.0 | . 2 -1.0 | -1.0 | -1.0 | 32549 | 3 | 7000.0 | . 3 0.0 | 2.0 | -1.0 | 11179 | 5 | 22000.0 | . 4 0.0 | 0.0 | 3.0 | 213095 | 4 | 3500.0 | . 5 -1.0 | -1.0 | -1.0 | 99213 | 4 | 4500.0 | . 6 1.0 | -1.0 | -1.0 | 45698 | 4 | 7500.0 | . 7 1.0 | -1.0 | -1.0 | 54738 | 4 | 7000.0 | . 8 -1.0 | -1.0 | -1.0 | 60000 | 4 | 6250.0 | . 9 0.0 | 0.0 | 3.0 | 31600 | 4 | 9700.0 | . helmert_encoding.mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: Make_0 Make_1 Make_2 1 -1.0 -1.0 -1.0 2 1.0 -1.0 -1.0 3 0.0 2.0 -1.0 4 0.0 0.0 3.0 -1 0.0 0.0 0.0 -2 0.0 0.0 0.0}] . Label Encoding . In this encoding, it assigns a number for each category, ranging from 1 to N. . The major issue with it is the numbers don&#39;t necessarly represent an order (Toyota &gt; Honda &gt; BMW &gt; Nissan). . from sklearn.preprocessing import LabelEncoder label_encoding = LabelEncoder() label_encoded_data = data.copy() label_encoded_data[&#39;Make&#39;] = label_encoding.fit_transform(data[&#39;Make&#39;]) label_encoded_data . Make Odometer (KM) Doors Price . 0 3 | 150043 | 4 | 4000.0 | . 1 1 | 87899 | 4 | 5000.0 | . 2 3 | 32549 | 3 | 7000.0 | . 3 0 | 11179 | 5 | 22000.0 | . 4 2 | 213095 | 4 | 3500.0 | . 5 3 | 99213 | 4 | 4500.0 | . 6 1 | 45698 | 4 | 7500.0 | . 7 1 | 54738 | 4 | 7000.0 | . 8 3 | 60000 | 4 | 6250.0 | . 9 2 | 31600 | 4 | 9700.0 | . label_encoding.classes_ . array([&#39;BMW&#39;, &#39;Honda&#39;, &#39;Nissan&#39;, &#39;Toyota&#39;], dtype=object) . Ordinal Encoding . It mostly works the same way as Label Encoding. Label encoding wouldn&#39;t consider whether the feature is ordinal or not. With ordinal encoding we provide what is the order of the categories in a column. . mapping = [{&quot;col&quot;: &quot;Make&quot;, &quot;mapping&quot;: {&quot;BMW&quot;: 1, &quot;Honda&quot;: 2, &quot;Nissan&quot;: 3, &quot;Toyota&quot;: 4}}] ordinal_encoding = ce.OrdinalEncoder(cols=[&quot;Make&quot;], mapping=mapping) ordinal_encoded_data = ordinal_encoding.fit_transform(data) ordinal_encoded_data . Make Odometer (KM) Doors Price . 0 4 | 150043 | 4 | 4000.0 | . 1 2 | 87899 | 4 | 5000.0 | . 2 4 | 32549 | 3 | 7000.0 | . 3 1 | 11179 | 5 | 22000.0 | . 4 3 | 213095 | 4 | 3500.0 | . 5 4 | 99213 | 4 | 4500.0 | . 6 2 | 45698 | 4 | 7500.0 | . 7 2 | 54738 | 4 | 7000.0 | . 8 4 | 60000 | 4 | 6250.0 | . 9 3 | 31600 | 4 | 9700.0 | . ordinal_encoding.category_mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: {&#39;BMW&#39;: 1, &#39;Honda&#39;: 2, &#39;Nissan&#39;: 3, &#39;Toyota&#39;: 4}}] . Label Encoding + Ordinal Encoding . such transformation should not be used “as is” for several types of models (Linear Models, KNN, Neural Nets, etc.). . | While applying gradient boosting it could be used only if the type of a column is specified as “category” . | . df[“category_representation”] = df[“category_representation”].astype(“category”) . If you are working with tabular data and your model is gradient boosting (especially LightGBM library), LE is the simplest and efficient way for you to work with categories in terms of memory (the category type in python consumes much less memory than the object type). | . Binary Encoding . Binary Encoding converts the number of categories N into a binary number, and uses every bit as a column. Here we have 4 categories which can be stored in 3 bits, meaning 3 columns. . binary_encoding = ce.BinaryEncoder(cols=&#39;Make&#39;) binary_encoded_data = binary_encoding.fit_transform(data) binary_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 0 | 0 | 1 | 150043 | 4 | 4000.0 | . 1 0 | 1 | 0 | 87899 | 4 | 5000.0 | . 2 0 | 0 | 1 | 32549 | 3 | 7000.0 | . 3 0 | 1 | 1 | 11179 | 5 | 22000.0 | . 4 1 | 0 | 0 | 213095 | 4 | 3500.0 | . 5 0 | 0 | 1 | 99213 | 4 | 4500.0 | . 6 0 | 1 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 1 | 0 | 54738 | 4 | 7000.0 | . 8 0 | 0 | 1 | 60000 | 4 | 6250.0 | . 9 1 | 0 | 0 | 31600 | 4 | 9700.0 | . Mapping: . Toyota: 001 | Honda: 010 | BMW: 011 | Nissan: 100 | . Frequency Encdoing . encoding for different sizes of test batch might be different. You should think about it beforehand and make preprocessing of the train as close to the test as possible. | . Nevertheless, Frequency Encoding and RFE are especially efficient when your categorical column has “long tails”, i.e. several frequent values and the remaining ones have only a few examples in the dataset. In such a case, Frequency Encoding would catch the similarity between rare columns. | . frequency_encoded_data = data.copy() freq = data.groupby(&#39;Make&#39;).size() frequency_encoded_data[&#39;Make&#39;] = data[&#39;Make&#39;].map(freq) frequency_encoded_data . Make Odometer (KM) Doors Price . 0 4 | 150043 | 4 | 4000.0 | . 1 3 | 87899 | 4 | 5000.0 | . 2 4 | 32549 | 3 | 7000.0 | . 3 1 | 11179 | 5 | 22000.0 | . 4 2 | 213095 | 4 | 3500.0 | . 5 4 | 99213 | 4 | 4500.0 | . 6 3 | 45698 | 4 | 7500.0 | . 7 3 | 54738 | 4 | 7000.0 | . 8 4 | 60000 | 4 | 6250.0 | . 9 2 | 31600 | 4 | 9700.0 | . Mean Encoding . Also Known as Target Encoding, is the go-to categorical encoding used in Kaggle competitions. | . It can figure out the categories that can have simillar effect on predict the target value. | . It doesn&#39;t affect the volume of the data, thus a faster learning process. . | The encoded category values are calculated according to the following formulas: . | . mdl — min data (samples) in leaf, a — smoothing parameter, representing the power of regularization. Recommended values for mdl and a are in the range of 1 to 100. | . It has a huge disadvantage — target leakage: it uses information about the target. Because of the target leakage, model overfits the training data which results in unreliable validation and lower test scores. | . To reduce the effect of target leakage, we may increase regularization (it’s hard to tune those hyperparameters without unreliable validation), | add random noise to the representation of the category in train dataset (some sort of augmentation), or | use Double Validation. | | . mean_encoding = ce.TargetEncoder(cols=&#39;Make&#39;, smoothing=0) mean_encoded_data = mean_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) mean_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] mean_encoded_data . Make Odometer (KM) Doors Price . 0 5437.5 | 150043 | 4 | 4000.0 | . 1 6500.0 | 87899 | 4 | 5000.0 | . 2 5437.5 | 32549 | 3 | 7000.0 | . 3 7645.0 | 11179 | 5 | 22000.0 | . 4 6600.0 | 213095 | 4 | 3500.0 | . 5 5437.5 | 99213 | 4 | 4500.0 | . 6 6500.0 | 45698 | 4 | 7500.0 | . 7 6500.0 | 54738 | 4 | 7000.0 | . 8 5437.5 | 60000 | 4 | 6250.0 | . 9 6600.0 | 31600 | 4 | 9700.0 | . M-estimator Encoding . M-Estimate Encoder is a simplified version of Target Encoder. It has only one hyperparameter — m, which represents the power of regularization | . . In different sources, you may find another formula of M-Estimator. Instead of y+ there is n in the denominator. I found that such representation has similar scores. | . m_estimator_encoding = ce.MEstimateEncoder(cols=&#39;Make&#39;, m=1) m_estimator_encoded_data = m_estimator_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) m_estimator_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] m_estimator_encoded_data . Make Odometer (KM) Doors Price . 0 5879.000000 | 150043 | 4 | 4000.0 | . 1 6786.250000 | 87899 | 4 | 5000.0 | . 2 5879.000000 | 32549 | 3 | 7000.0 | . 3 14822.500000 | 11179 | 5 | 22000.0 | . 4 6948.333333 | 213095 | 4 | 3500.0 | . 5 5879.000000 | 99213 | 4 | 4500.0 | . 6 6786.250000 | 45698 | 4 | 7500.0 | . 7 6786.250000 | 54738 | 4 | 7000.0 | . 8 5879.000000 | 60000 | 4 | 6250.0 | . 9 6948.333333 | 31600 | 4 | 9700.0 | . Weight of Evidence Encoding . Probability Ratio Encoding . both are very similar, and both are explanied in this article. . WOE and Probability Ratio both works with binary classification problems. . i won&#39;t go over them because i can&#39;t grasp it completly. even though i understand how the code works and the instructions of the algorithm. i can&#39;t completly understand how it can affect the learning process. . Weight of Evidence Encdoing . is a commonly used target-based encoder in credit scoring. | . It is a measure of the “strength” of a grouping for separating good and bad risk. | . it might lead to target leakage and overfit. To avoid that, regularization parameter a is induced and WoE is calculated in the following way | . . if is_binary_classification: woe_encoding = ce.WOEEncoder(cols=&#39;Make&#39;) woe_encoded_data = woe_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) woe_encoded_data . Hashing . With Hashing, the number of dimensions will be far less than the number of dimensions with encoding like One Hot Encoding. This method is advantageous when the cardinality of categorical is very high. . hashing_encoding = ce.HashingEncoder(cols=&#39;Make&#39;) hashing_encoded_data = hashing_encoding.fit_transform(data) hashing_encoded_data . col_0 col_1 col_2 col_3 col_4 col_5 col_6 col_7 Odometer (KM) Doors Price . 0 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 150043 | 4 | 4000.0 | . 1 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 87899 | 4 | 5000.0 | . 2 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 32549 | 3 | 7000.0 | . 3 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11179 | 5 | 22000.0 | . 4 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 213095 | 4 | 3500.0 | . 5 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 99213 | 4 | 4500.0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 45698 | 4 | 7500.0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 54738 | 4 | 7000.0 | . 8 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 60000 | 4 | 6250.0 | . 9 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 31600 | 4 | 9700.0 | . Backward Difference Encdoing . This Tequnique falls under the contrast coding system for categorical features. | It creates N-1 columns (N is no. of columns) | It compares the mean of the dependent variable with the mean of the dependent variable for the prior level | . bd_encoding = ce.BackwardDifferenceEncoder(cols=&#39;Make&#39;, drop_invariant=True) bd_encoded_data = bd_encoding.fit_transform(data) bd_encoded_data . Make_0 Make_1 Make_2 Odometer (KM) Doors Price . 0 -0.75 | -0.5 | -0.25 | 150043 | 4 | 4000.0 | . 1 0.25 | -0.5 | -0.25 | 87899 | 4 | 5000.0 | . 2 -0.75 | -0.5 | -0.25 | 32549 | 3 | 7000.0 | . 3 0.25 | 0.5 | -0.25 | 11179 | 5 | 22000.0 | . 4 0.25 | 0.5 | 0.75 | 213095 | 4 | 3500.0 | . 5 -0.75 | -0.5 | -0.25 | 99213 | 4 | 4500.0 | . 6 0.25 | -0.5 | -0.25 | 45698 | 4 | 7500.0 | . 7 0.25 | -0.5 | -0.25 | 54738 | 4 | 7000.0 | . 8 -0.75 | -0.5 | -0.25 | 60000 | 4 | 6250.0 | . 9 0.25 | 0.5 | 0.75 | 31600 | 4 | 9700.0 | . Looks very similar to Helmert Encoding . bd_encoding.mapping . [{&#39;col&#39;: &#39;Make&#39;, &#39;mapping&#39;: Make_0 Make_1 Make_2 1 -0.75 -0.5 -0.25 2 0.25 -0.5 -0.25 3 0.25 0.5 -0.25 4 0.25 0.5 0.75 -1 0.00 0.0 0.00 -2 0.00 0.0 0.00}] . Leave One Out Encoding . This is very similar to target encoding but excludes the current row’s target when calculating the mean target for a level to reduce outliers. . loo_encoding = ce.LeaveOneOutEncoder(cols=&#39;Make&#39;) loo_encoded_data = loo_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) loo_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] loo_encoded_data . Make Odometer (KM) Doors Price . 0 5916.666667 | 150043 | 4 | 4000.0 | . 1 7250.000000 | 87899 | 4 | 5000.0 | . 2 4916.666667 | 32549 | 3 | 7000.0 | . 3 7645.000000 | 11179 | 5 | 22000.0 | . 4 9700.000000 | 213095 | 4 | 3500.0 | . 5 5750.000000 | 99213 | 4 | 4500.0 | . 6 6000.000000 | 45698 | 4 | 7500.0 | . 7 6250.000000 | 54738 | 4 | 7000.0 | . 8 5166.666667 | 60000 | 4 | 6250.0 | . 9 3500.000000 | 31600 | 4 | 9700.0 | . James-Stein Encoding . Target-based Encoder . | Returns a weighted Average of: . The mean target value for the observed feature value. | The mean for the whole target value | . | It tends to shrinks the average toard the overall average. . | One Major issue, it was defined only for normal distributions . | . if is_normally_distributed: james_stein_encoding = ce.JamesSteinEncoder(cols=[&#39;Make&#39;]) james_stein_encoded_data = james_stein_encoding.fit_transform(data.drop(&#39;Price&#39;, axis=1), data[&#39;Price&#39;]) james_stein_encoded_data[&#39;Price&#39;] = data[&#39;Price&#39;] james_stein_encoded_data . Which encoding method to use for your dataset? . It&#39;s recommended to follow the following chart to figure out which one best suited for your dataset. . You can test on multiple methods on a sample of your dataset, it should give you a good view of which one is better for your situation. . . from sklearn.linear_model import LinearRegression results = {} encoded_datasets = { &quot;One Hot Encoding&quot;: one_hot_encoded_data, &quot;Label Encoding&quot;: label_encoded_data, &quot;Ordinal Encoding&quot;: ordinal_encoded_data, &quot;Helmert Encoding&quot;: helmert_encoded_data, &quot;Binary Encoding&quot;: binary_encoded_data, &quot;Frequency Encoding&quot;: frequency_encoded_data, &quot;Mean Encoding&quot;: mean_encoded_data, &quot;Hashing&quot;: hashing_encoded_data, &quot;Backward Difference Encoding&quot;: bd_encoded_data, &quot;Leave-One-Out&quot;: loo_encoded_data, &quot;M-Estimator Encoding&quot;: m_estimator_encoded_data, } if is_normally_distributed: encoded_datasets[&quot;James-Stein Encoding&quot;] = james_stein_encoded_data if is_binary_classification: encoded_datasets[&quot;Weight of Evidence Encoding&quot;] = woe_encoded_data for encoding_name, encoded_dataset in encoded_datasets.items(): # Split data X = encoded_dataset.drop(&#39;Price&#39;, axis=1) y = encoded_dataset[&#39;Price&#39;] model = LinearRegression() model.fit(X, y) score = model.score(X, y) results[encoding_name] = score; . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . LinearRegression() . sorted(results.items(), key=lambda x:x[1], reverse=True) . [(&#39;One Hot Encoding&#39;, 0.9945536929215048), (&#39;Helmert Encoding&#39;, 0.9945536929215048), (&#39;Binary Encoding&#39;, 0.9945536929215048), (&#39;Hashing&#39;, 0.9945536929215048), (&#39;Backward Difference Encoding&#39;, 0.9945536929215048), (&#39;M-Estimator Encoding&#39;, 0.9801482890347663), (&#39;Frequency Encoding&#39;, 0.8508034307880376), (&#39;Leave-One-Out&#39;, 0.8166612326802211), (&#39;Mean Encoding&#39;, 0.813764093905115), (&#39;Label Encoding&#39;, 0.7735336830068426), (&#39;Ordinal Encoding&#39;, 0.7735336830068424)] . Notes . Encoders-wise . LE and OE are bad with Linear Models, KNN, and Neural Nets . | OHE is good for tree-based algorithms(and for regression if it produces N-1 columns instead of N) . | Helmert Encoding and Sum Encoding is good for Linear models . | . Algorithm-wise . With Regression, SVM, NN, and clusting algorithms, it&#39;s better to use an encoder that generates N-1 columns | . Resources . Benchmarking Categorical Encoders . All About Categorical Variable Encoding . To-Read . Coding Categorical Variables (Research Paper) .",
            "url": "https://o-hatem.github.io/My-Journey/jupyter/2022/01/22/Categorical-Encoding.html",
            "relUrl": "/jupyter/2022/01/22/Categorical-Encoding.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Stuff to Learn",
            "content": "Python Library . datatable. a python library that help loads files into notebooks faster than doing it normally with pd.read_csv(). This a link to sets, each represent a topic on the library. . | gc. Python’s garbage collector module. . | category_encoders. More specific a coder named TargetEncoder . | Useful Concepts . Memory Reduction. For large datasets, Memor reduction is very useful in saving processing time. Found in this notebook . | Using manual cross validation functions. . | Machine Learning Algorithms . XGBoost. |",
            "url": "https://o-hatem.github.io/My-Journey/markdown/2021/10/27/NewStuffToLearn.html",
            "relUrl": "/markdown/2021/10/27/NewStuffToLearn.html",
            "date": " • Oct 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "My Roadmap for Becoming a Data Scientist",
            "content": "RoadMap . Data Science and Machine Learning Udemy course. (finished in about 3 months) . Just today (Oct 25th, 2021), i almost the couse. It was very since and taught me how to write code for machine learning models with a small transefer learning model in the end. I learned how to : . Preprocess Data (Cleaning, Imputing, etc) | Use Numpy, Pandas for accessing and manipulating data in dataframes | Use Matplotlib to visualize data and figure out how my model is performing | Write an end-to-end machine learning models for regression and classification problems. | Use TensorFlow and TensorFlow Hub with Keras to build a transefer learning model | . but the course lackes the theortical part of machine learning and deep learning which is what i am looking to do next. . | Mathematics for Machine Leaning Coursea course . | What i’m looking to do next . Andrew Ng Youtube playlist . Data Science from Scratch Book from O’reilly . Notes . I should not stop doing Kaggle comeptitions and should be coming up with my own ideas while taking a therotical course. I’ll feel discouraged to compelete the course if all i see is the math and equations. And to not forget the practical part. | . Storytelling . (Oct 27th, 2021) After a little bit of thinking i decided to start with Andrew Ng’s Machine Learning Playlist on YouTube. But before that, i’d like to review all the math needed to complete the course which include: Linear Algebra | Multi-variable calculas | Probability and Statistics | . The Mathematics for Machine Learning Coursera course covers the first two points plus PCA (principal component analysis). so after finishing it, i’ll search for a convinent course for probability and statistics for machine learning. . | (Jan 12th, 2021) I found a good youtube playlist for probability and statistics and i think it’s pretty good. videos are short and it covers a lot of topics. link. That’s a website with a several probability serieses. link There is a reference file about probability in Andrew Ng course . | A nice deep learning playlist i found on youtube with all the material. link | . Deadlines . Finish the ZTM:Data Science and Machine Learning course Start on: (idk exactly but around August) | Deadline on: Nov 25th, 2021 | Finished on: Oct 25th, 2021 (one month early) | . | Mathematics for Machine Learning: Linear Algebra (~18h course) Start on: Oct 27th, 2021 | Deadline on: Nov 27th, 2021 | Finished on: Dec 3rd, 2021 (one week late) | . | Mathematics for Machine Learning: Multivariate Calculas (~18h course) Start on: Dec 4th, 2021 | Deadline on: Jan 17th, 2021 | Finished on: TBD | . | .",
            "url": "https://o-hatem.github.io/My-Journey/markdown/2021/10/25/RoadMap.html",
            "relUrl": "/markdown/2021/10/25/RoadMap.html",
            "date": " • Oct 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Welcome to my portifolio . I’m an aspiring data scientist. I found my passion for data science and machine learning in college. And since then i decided to i want to beome one. . What i love about data science is the curiosity and the search for answers for demanding questions that can geniuly help people. Add this to my passion and love for exploration for answer to my questions, i think i found a career i’ll never reget choosing. . I love and read in sociology and i find it intersiting to see how a society behave and how can a group of people (like in a company) take decisions. pairing that with the data science skills i have, i belive i can find some answers. .",
          "url": "https://o-hatem.github.io/My-Journey/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "Welcome to my portifolio . I’m an aspiring data scientist. I found my passion for data science and machine learning in college. And since then i decided to i want to beome one. . What i love about data science is the curiosity and the search for answers for demanding questions that can geniuly help people. Add this to my passion and love for exploration for answer to my questions, i think i found a career i’ll never reget choosing. . I love and read in sociology and i find it intersiting to see how a society behave and how can a group of people (like in a company) take decisions. pairing that with the data science skills i have, i belive i can find some answers. . Posts .",
          "url": "https://o-hatem.github.io/My-Journey/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://o-hatem.github.io/My-Journey/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}